{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurent_Data = pd.read_csv(\"C:\\\\Users\\\\graykar\\\\Downloads\\\\Pyhton\\\\Data Files\\\\ResturentSentiment_Analysis.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't Love it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I do not Love it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The selection on the menu was great and so were the prices.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    Review  \\\n",
       "0                                                                               I love it.   \n",
       "1                                                                       I don't Love it...   \n",
       "2                                                                      I do not Love it...   \n",
       "3                                                                 Wow... Loved this place.   \n",
       "4                                                                       Crust is not good.   \n",
       "5                                                Not tasty and the texture was just nasty.   \n",
       "6  Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.   \n",
       "7                              The selection on the menu was great and so were the prices.   \n",
       "8                                           Now I am getting angry and I want my damn pho.   \n",
       "9                                                    Honeslty it didn't taste THAT fresh.)   \n",
       "\n",
       "   Liked  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  \n",
       "5      0  \n",
       "6      1  \n",
       "7      1  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Restaurent_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\graykar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Restaurent_Data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data = []\n",
    "LMT = WordNetLemmatizer()\n",
    "\n",
    "for i in range(Restaurent_Data.shape[0]):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', Restaurent_Data['Review'][i] ).lower().split() \n",
    "    review = [LMT.lemmatize(x) for x in review if not x in set(stopwords.words('english')) - {'not'}]\n",
    "    review = \" \".join(review)\n",
    "    converted_data.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\graykar\\Documents\\Annaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import  one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_representation = [one_hot(word,vocabulary_size) for word in converted_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "17\n",
      "17\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for i in one_hot_representation:\n",
    "    if len(i) > 15:\n",
    "        print(len(i))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_lenght = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0 7582]\n",
      " [   0    0    0 ...    0    0 7582]\n",
      " [   0    0    0 ...    0 8320 7582]\n",
      " ...\n",
      " [   0    0    0 ... 8320 7119 3904]\n",
      " [   0    0    0 ... 3450 4203 5028]\n",
      " [   0    0    0 ... 4160 6662 2051]]\n"
     ]
    }
   ],
   "source": [
    "Embeded_doc = pad_sequences(one_hot_representation, maxlen=string_lenght, padding='pre')\n",
    "print(Embeded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 25)            250000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 25)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               50400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 300,501\n",
      "Trainable params: 300,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vec_dimension = 25\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, vec_dimension , input_length=string_lenght ))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, (1003,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Embeded_doc)) , Restaurent_Data['Liked'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.array(Embeded_doc)\n",
    "Y_final = np.array(Restaurent_Data['Liked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1003, 20), (1003,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape , Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X_final , Y_final , test_size=0.25, random_state=48 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((752, 20), (752,), (251, 20), (251,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape,  X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 752 samples, validate on 251 samples\n",
      "Epoch 1/25\n",
      "752/752 [==============================] - 4s 5ms/sample - loss: 0.6898 - accuracy: 0.5266 - val_loss: 0.6784 - val_accuracy: 0.6335\n",
      "Epoch 2/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.5491 - accuracy: 0.7739 - val_loss: 0.5102 - val_accuracy: 0.7928\n",
      "Epoch 3/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.3060 - accuracy: 0.8976 - val_loss: 0.5202 - val_accuracy: 0.7371\n",
      "Epoch 4/25\n",
      "752/752 [==============================] - 2s 2ms/sample - loss: 0.1716 - accuracy: 0.9441 - val_loss: 0.5189 - val_accuracy: 0.7331\n",
      "Epoch 5/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.1057 - accuracy: 0.9668 - val_loss: 0.5977 - val_accuracy: 0.7809\n",
      "Epoch 6/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0826 - accuracy: 0.9774 - val_loss: 0.5937 - val_accuracy: 0.7610\n",
      "Epoch 7/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0536 - accuracy: 0.9827 - val_loss: 0.7218 - val_accuracy: 0.7570\n",
      "Epoch 8/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0338 - accuracy: 0.9907 - val_loss: 0.7721 - val_accuracy: 0.7610\n",
      "Epoch 9/25\n",
      "752/752 [==============================] - 2s 3ms/sample - loss: 0.0319 - accuracy: 0.9907 - val_loss: 0.8013 - val_accuracy: 0.7729\n",
      "Epoch 10/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0654 - accuracy: 0.9814 - val_loss: 0.8785 - val_accuracy: 0.6813\n",
      "Epoch 11/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.1126 - accuracy: 0.9681 - val_loss: 0.8671 - val_accuracy: 0.7490\n",
      "Epoch 12/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0711 - accuracy: 0.9814 - val_loss: 0.9919 - val_accuracy: 0.7530\n",
      "Epoch 13/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0489 - accuracy: 0.9867 - val_loss: 1.0863 - val_accuracy: 0.6932\n",
      "Epoch 14/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0506 - accuracy: 0.9867 - val_loss: 0.8434 - val_accuracy: 0.7649\n",
      "Epoch 15/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0277 - accuracy: 0.9920 - val_loss: 0.8891 - val_accuracy: 0.7610\n",
      "Epoch 16/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0196 - accuracy: 0.9934 - val_loss: 0.9093 - val_accuracy: 0.7610\n",
      "Epoch 17/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0209 - accuracy: 0.9920 - val_loss: 0.9270 - val_accuracy: 0.7649\n",
      "Epoch 18/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0174 - accuracy: 0.9920 - val_loss: 0.9348 - val_accuracy: 0.7769\n",
      "Epoch 19/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0192 - accuracy: 0.9920 - val_loss: 0.9707 - val_accuracy: 0.7729\n",
      "Epoch 20/25\n",
      "752/752 [==============================] - 1s 1ms/sample - loss: 0.0165 - accuracy: 0.9934 - val_loss: 0.9888 - val_accuracy: 0.7649\n",
      "Epoch 21/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0141 - accuracy: 0.9920 - val_loss: 1.0257 - val_accuracy: 0.7968\n",
      "Epoch 22/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0148 - accuracy: 0.9947 - val_loss: 1.0190 - val_accuracy: 0.7610\n",
      "Epoch 23/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0138 - accuracy: 0.9947 - val_loss: 1.0489 - val_accuracy: 0.7649\n",
      "Epoch 24/25\n",
      "752/752 [==============================] - 2s 2ms/sample - loss: 0.0134 - accuracy: 0.9934 - val_loss: 1.0424 - val_accuracy: 0.7928\n",
      "Epoch 25/25\n",
      "752/752 [==============================] - 1s 2ms/sample - loss: 0.0147 - accuracy: 0.9934 - val_loss: 1.0701 - val_accuracy: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd2b320>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train , y_train , epochs=25 , validation_data=(X_test,y_test), batch_size=8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[112  26]\n",
      " [ 27  86]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7888446215139442"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "f1_score(y_test, y_pred)\n",
    "accuracy_score (y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
